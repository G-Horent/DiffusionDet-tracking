{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nGP5IkuVH5a"
   },
   "source": [
    "# Object tracking with DiffusionDet\n",
    "\n",
    "## Author : Guillaume Horent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWMMv9arVKC8"
   },
   "source": [
    "## Installation of all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ha6-0moaVOOu",
    "outputId": "de613e5e-406e-4ac2-9469-a82fb325b3cf"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data.detection_utils import read_image\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.modeling import detector_postprocess\n",
    "from torchvision.ops import nms\n",
    "\n",
    "from diffusiondet.config import add_diffusiondet_config\n",
    "from diffusiondet.predictor import VisualizationDemo\n",
    "from diffusiondet.util.model_ema import add_model_ema_configs, may_build_model_ema, may_get_ema_checkpointer, EMAHook, \\\n",
    "    apply_model_ema_and_restore, EMADetectionCheckpointer\n",
    "\n",
    "from diffusiondet.detector import DiffusionDet as ddet\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import csv\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-NPZf_5EVQi9"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EbWYXmYVUnh"
   },
   "source": [
    "# Configuration of DiffusionDet\n",
    "\n",
    "Here we use a pretrained DiffusionDet which uses a ResNet101 trained on MS-COCO.(https://github.com/ShoufaChen/DiffusionDet). \n",
    "\n",
    "We use a confidence_threshold equal to 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CAo3GcJSVWJ1"
   },
   "outputs": [],
   "source": [
    "#Configuration of the DiffusionDet with Resnet101 trained on MS-COCO\n",
    "\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "def setup_cfg():#args):\n",
    "  # load config from file and command-line arguments\n",
    "  cfg = get_cfg()\n",
    "  # To use demo for Panoptic-DeepLab, please uncomment the following two lines.\n",
    "  # from detectron2.projects.panoptic_deeplab import add_panoptic_deeplab_config  # no need\n",
    "  # add_panoptic_deeplab_config(cfg)\n",
    "  add_diffusiondet_config(cfg)\n",
    "  add_model_ema_configs(cfg)\n",
    "  cfg.merge_from_file('configs/diffdet.mot17.swinbase.yaml')\n",
    "  cfg.MODEL.WEIGHTS = 'models/diffdet_coco_swinbase.pth'\n",
    "  cfg.MODEL.DiffusionDet.USE_NMS = True\n",
    "  #cfg.merge_from_list(args.opts)\n",
    "  # Set score_threshold for builtin models\n",
    "  cfg.MODEL.RETINANET.SCORE_THRESH_TEST = confidence_threshold\n",
    "  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = confidence_threshold\n",
    "  cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = confidence_threshold\n",
    "  cfg.MODEL.DiffusionDet.NUM_PROPOSALS = 500 # To Adapt\n",
    "  cfg.MODEL.DiffusionDet.SAMPLE_STEP = 1 # To Adapt\n",
    "  cfg.freeze()\n",
    "  return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eioRSewtVZIG"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_iou(bbox1, bbox2):\n",
    "  #Returns the ratio of the intersection over the union of the two boxes\n",
    "\n",
    "  # Compute the coordinates of the intersection rectangle\n",
    "  x_left = max(bbox1[0], bbox2[0])\n",
    "  y_top = max(bbox1[1], bbox2[1])\n",
    "  x_right = min(bbox1[2], bbox2[2])\n",
    "  y_bottom = min(bbox1[3], bbox2[3])\n",
    "\n",
    "  # Check if the intersection is valid\n",
    "  if x_right < x_left or y_bottom < y_top:\n",
    "      return 0.0\n",
    "\n",
    "  # Compute the area of the intersection\n",
    "  intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "  # Compute the areas of the bounding boxes\n",
    "  bbox1_area = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
    "  bbox2_area = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
    "\n",
    "  # Compute the IoU\n",
    "  iou = intersection_area / (bbox1_area + bbox2_area - intersection_area)\n",
    "  return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyPMzcDlVamS"
   },
   "source": [
    "## Object tracking on a video file\n",
    "\n",
    "We first configure the DiffusionDet with the previously imported weights and parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "76DcD5uSVdAY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#Configuration of the DiffusionDet\n",
    "cfg = setup_cfg()\n",
    "demo = VisualizationDemo(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjzNFNodVe5_"
   },
   "source": [
    "Then we load the folder containing the videos and intialize the frame numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcqxITC4ViJr"
   },
   "source": [
    "In the following we iterate over every frame of every videos. \n",
    "For each frame :\n",
    "- we detect the bounding boxes\n",
    "- we add the boxes to the tracks\n",
    "- for each box:\n",
    "  - find the one that maximises IoU (Intersection over Union)\n",
    "    - if the maximum IoU is over 0.5 than we consider that it is the same box\n",
    "    - otherwise, create a new box with an unused ID\n",
    "\n",
    "  - print the corresponding box on the frame\n",
    "- add the frame to the tracked video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofNPwbioVlhk"
   },
   "outputs": [],
   "source": [
    "tracks = {}\n",
    "# Initialize the dictionary of track IDs\n",
    "track_ids = {}\n",
    "\n",
    "for video in videos:\n",
    "    vid_path = os.path.join(video_dir, video)\n",
    "    cap = cv2.VideoCapture(vid_path)\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_name = video.split(\".\")[0] + \"_tracked.mp4\"\n",
    "    video_path = os.path.join(\"/content/drive/MyDrive/DiffusionDet/diffusiondet/tracked_videos\", video_name)\n",
    "    out = cv2.VideoWriter(video_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Iterate through frames in the video\n",
    "    while success:\n",
    "        # Extract bounding boxes, labels, and probabilities using demo.run_on_image\n",
    "        # Replace this with your own code for running object detection\n",
    "        \n",
    "        if frame_number not in tracks:\n",
    "            tracks[frame_number] = []\n",
    "\n",
    "        print(str(frame_number) + '/301')\n",
    "        detections = demo.run_on_image(frame)\n",
    "        # Format the detections for convenience\n",
    "        # Replace this with your own code for formatting the detections\n",
    "\n",
    "        bboxes = detections[0]['instances']._fields['pred_boxes'].tensor\n",
    "        labels = detections[0]['instances']._fields['pred_classes']\n",
    "        scores = detections[0]['instances']._fields['scores']\n",
    "        keep = nms(bboxes, scores, 0.5)\n",
    "        bboxes = bboxes[keep]\n",
    "        scores = scores[keep]\n",
    "        labels = labels[keep]\n",
    "\n",
    "        for bbox, label, score in zip(bboxes, labels, scores):\n",
    "            # Initialize a new track\n",
    "            track = {'bbox': bbox, 'conf': score}\n",
    "\n",
    "            # Find the closest track in the previous frame based on the IoU\n",
    "            max_iou = 0\n",
    "            best_match = None\n",
    "            if frame_number > 1 and tracks[frame_number - 1]:\n",
    "                for previous_track in tracks[frame_number - 1]:\n",
    "                    iou = compute_iou(bbox, previous_track['bbox'])\n",
    "                    if iou > max_iou:\n",
    "                        max_iou = iou\n",
    "                        best_match = previous_track\n",
    "                        \n",
    "            if max_iou > 0.5:\n",
    "                # Update the track with the new bounding box and confidence\n",
    "                best_match['bbox'] = bbox\n",
    "                best_match['conf'] = score\n",
    "                if 'id' in track:  # check if the track already has an ID\n",
    "                    track['id'] = track['id']  # use the existing ID\n",
    "                else:\n",
    "                    track['id'] = best_match['id']  # assign the ID of the best matching track from the previous frame\n",
    "            else:\n",
    "                if track_ids:  # check if the dictionary of track IDs is not empty\n",
    "                    # Assign a new ID that hasn't been used yet\n",
    "                    unused_ids = [id for id in track_ids if id not in track_ids.values()]\n",
    "                    if unused_ids:  # check if there are any unused IDs\n",
    "                        track['id'] = unused_ids[0]\n",
    "                        track_ids.update({track['id']: track['id']})  # add the new ID to the dictionary of track IDs\n",
    "                    else:\n",
    "                        track['id'] = max(track_ids) + 1  # assign a new ID\n",
    "                        track_ids.update({track['id']: track['id']})  # add the new ID to the dictionary of track IDs\n",
    "                else:\n",
    "                    track['id'] = 1\n",
    "                    track_ids.update({track['id']: track['id']})  # add the new ID to the dictionary of track IDs\n",
    "            tracks[frame_number].append(track)  # add the track to the list of tracks\n",
    "\n",
    "        colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255),(128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0), (128, 0, 128), (0, 128, 128),           (64, 0, 0), (0, 64, 0), (0, 0, 64)]\n",
    "\n",
    "        for track in tracks[frame_number]:\n",
    "            xmin, ymin, xmax, ymax = track['bbox']\n",
    "            color = colors[track['id'] % len(colors)]\n",
    "            cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, 2)\n",
    "            cv2.putText(frame, str(track['id']) + ' ' + str(\"{:.2f}\".format(track['conf'].item())), (int(xmin), int(ymin) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "        success, frame = cap.read()\n",
    "        frame_number += 1\n",
    "\n",
    "    # Release the VideoCapture and VideoWriter objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-0WCOhFV15w"
   },
   "source": [
    "## Object Tracking with Diffusion Det\n",
    "\n",
    "In this section we perform the object tracking task on a video from MOT17. While we perform the object tracking task, we also write a csv file with the following information for every frame : \n",
    "\n",
    "frame, id, bb_left, bb_top, bb_width, bb_height, conf, x, y, z\n",
    "\n",
    "or\n",
    "\n",
    "frame, id, bb_left, bb_top, bb_width, bb_height, conf\n",
    "\n",
    "\n",
    "In the previous line, we have : \n",
    "- frame: the frame number\n",
    "- id: the class if of the box (a \"car\" for instance)\n",
    "- bb_left: left side of the box x-axis coordinate\n",
    "- bb_top: upper side of the box y-axis coordinate\n",
    "- bb_width: width of the box\n",
    "- bb_height : height of the box\n",
    "- conf : confidence detection\n",
    "- x,y and z : they can be ignored in a 2D detection model\n",
    "\n",
    "In the next section, we consider MOT17-02-SDP which doesn't includes the x,y,z coordinate in the csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EifR91RkV1S1"
   },
   "outputs": [],
   "source": [
    "def object_tracking_frame_txt_writing(frames_folder,video_name,detection_file_name, reverse=False,video_output=True):\n",
    "    #Load the file of interest : (worked on this one because it is one of the shortest of the MOT17)\n",
    "    frames_dir = \"datasets/MOT17/train/\"+ frames_folder\n",
    "    frames_files_list = [f for f in os.listdir(frames_dir) if f.endswith(\".jpg\")]\n",
    "    frames_files_list.sort(reverse=reverse)\n",
    "    output_folder = 'results_proposals_500'\n",
    "  \n",
    "\n",
    "    #We initialise the frame number\n",
    "    frame_number = 1\n",
    "  \n",
    "    # Initialize the dictionary of track IDs\n",
    "    tracks = {}\n",
    "    track_ids = {}\n",
    "\n",
    "    #We initialise the detection function\n",
    "    detections_list = []\n",
    "\n",
    "    #We initialize the output video\n",
    "    if video_output==True:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video_path = os.path.join(output_folder, video_name)\n",
    "        frame_0 = cv2.imread(frames_dir + '/' + frames_files_list[0])\n",
    "        out = cv2.VideoWriter(video_path, fourcc, 30, (frame_0.shape[1], frame_0.shape[0]))\n",
    "\n",
    "    #We intialize the csv files that writes the boxes detected\n",
    "    with open(output_folder + '/' + detection_file_name, 'w', newline='') as txtfile:\n",
    "    \n",
    "        # Iterate through frames in the video\n",
    "        for i,frame_name in enumerate(tqdm(frames_files_list)):\n",
    "            frame = cv2.imread(frames_dir+'/'+frame_name)\n",
    "              #print(frame_name)\n",
    "\n",
    "            if frame_number not in tracks:\n",
    "                  tracks[frame_number] = []\n",
    "\n",
    "            #print(str(frame_number) + '/' +str(len(frames_files_list)))\n",
    "\n",
    "            detections = demo.run_on_image(frame)\n",
    "            # Format the detections for convenience\n",
    "            # Replace this with your own code for formatting the detections\n",
    "            bboxes = detections[0]['instances']._fields['pred_boxes'].tensor\n",
    "            labels = detections[0]['instances']._fields['pred_classes']\n",
    "            scores = detections[0]['instances']._fields['scores']\n",
    "            # keep only detections of pedestrians\n",
    "            idx_pedestrians = torch.squeeze((labels == 0).nonzero(), dim=1)\n",
    "            bboxes = bboxes[idx_pedestrians, :]\n",
    "            scores = scores[idx_pedestrians]\n",
    "            labels = labels[idx_pedestrians]\n",
    "            \n",
    "            keep = nms(bboxes, scores, 0.5)\n",
    "            bboxes = bboxes[keep]\n",
    "            scores = scores[keep]\n",
    "            labels = labels[keep]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for bbox, label, score in zip(bboxes, labels, scores):\n",
    "                # Initialize a new track\n",
    "                track = {'bbox': bbox, 'conf': score}\n",
    "\n",
    "                # Find the closest track in the previous frame based on the IoU\n",
    "                max_iou = 0\n",
    "                best_match = None\n",
    "                if frame_number > 1 and tracks[frame_number - 1]:\n",
    "                    for previous_track in tracks[frame_number - 1]:\n",
    "                        iou = compute_iou(bbox, previous_track['bbox'])\n",
    "                        if iou > max_iou:\n",
    "                            max_iou = iou\n",
    "                            best_match = previous_track\n",
    "\n",
    "                if max_iou > 0.5:\n",
    "                    # Update the track with the new bounding box and confidence\n",
    "                    best_match['bbox'] = bbox\n",
    "                    best_match['conf'] = score\n",
    "                    if 'id' in track:  # check if the track already has an ID\n",
    "                        track['id'] = track['id']  # use the existing ID\n",
    "                    else:\n",
    "                        track['id'] = best_match['id']  # assign the ID of the best matching track from the previous frame\n",
    "                else:\n",
    "                    if track_ids:  # check if the dictionary of track IDs is not empty\n",
    "                        # Assign a new ID that hasn't been used yet\n",
    "                        unused_ids = [id for id in track_ids if id not in track_ids.values()]\n",
    "                        if unused_ids:  # check if there are any unused IDs\n",
    "                            track['id'] = unused_ids[0]\n",
    "                            track_ids.update({track['id']: track['id']})  # add the new ID to the dictionary of track IDs\n",
    "                        else:\n",
    "                            track['id'] = max(track_ids) + 1  # assign a new ID\n",
    "                            track_ids.update({track['id']: track['id']})  # add the new ID to the dictionary of track IDs\n",
    "                    else:\n",
    "                        track['id'] = 1\n",
    "                        track_ids.update({track['id']: track['id']})  # add the new ID to the dictionary of track IDs\n",
    "                tracks[frame_number].append(track)  # add the track to the list of tracks\n",
    "\n",
    "                #Now we write each box in the file\n",
    "                #frame, id, bb_left, bb_top, bb_width, bb_height, conf, x, y, z\n",
    "                bbox_cpu = bbox.to(torch.device('cpu')).numpy()\n",
    "                label_cpu = label.to(torch.device('cpu')).numpy()\n",
    "                score_cpu = score.to(torch.device('cpu')).numpy()\n",
    "                width_bbox = np.abs(bbox_cpu[0]-bbox_cpu[2])\n",
    "                height_bbox = np.abs(bbox_cpu[1]-bbox_cpu[3])\n",
    "                if(reverse==False):\n",
    "                    txtfile.write(str(frame_number)+','+str(label_cpu)+','+str(bbox_cpu[0])+','+str(bbox_cpu[1])+','+str(width_bbox)+','+str(height_bbox)+','+str(track['id'])+','+str(score_cpu)+ '\\n')\n",
    "                    detections_list.append([frame_number,int(label_cpu),bbox_cpu[0],bbox_cpu[1],width_bbox,height_bbox,float(score_cpu)])\n",
    "                if(reverse==True):\n",
    "                    txtfile.write(str(len(frames_files_list)-frame_number+1)+','+str(label_cpu)+','+str(bbox_cpu[0])+','+str(bbox_cpu[1])+','+str(width_bbox)+','+str(height_bbox)+','+str(track['id'])+','+str(score_cpu)+ '\\n')\n",
    "                    detections_list.append([len(frames_files_list)-frame_number+1,int(label_cpu),bbox_cpu[0],bbox_cpu[1],width_bbox,height_bbox,float(score_cpu)])\n",
    "            #We only create a video output if required\n",
    "            if video_output==True:\n",
    "                colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255),(128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0), (128, 0, 128), (0, 128, 128),(64, 0, 0), (0, 64, 0), (0, 0, 64)]\n",
    "                for track in tracks[frame_number]:\n",
    "                    xmin, ymin, xmax, ymax = track['bbox']\n",
    "                    color = colors[track['id'] % len(colors)]\n",
    "                    cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, 2)\n",
    "                    cv2.putText(frame, str(track['id']) + ' ' + str(\"{:.2f}\".format(track['conf'].item())), (int(xmin), int(ymin) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                out.write(frame)\n",
    "\n",
    "            frame_number += 1\n",
    "    if(video_output==True):\n",
    "        out.release() \n",
    "    return detections_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-h5V4h1wo2Y"
   },
   "source": [
    "Now we test the above function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RK_K3g8xtgaq",
    "outputId": "45219b05-d522-4df0-ea69-685af0e50ed4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 750/750 [09:06<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "test_detections = object_tracking_frame_txt_writing(frames_folder='MOT17-13-DPM/img1',video_name = \"13_DPM_tracked_new_test.mp4\",\n",
    "                                  detection_file_name='13_DPM_reversed_det.txt',reverse=False,video_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQiJsLlEwwmA"
   },
   "source": [
    "##Performance evaluation\n",
    "\n",
    "In order to compare the performance of the model we get the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "uhEzv4Ku0yWB"
   },
   "outputs": [],
   "source": [
    "# Load the ground truth annotations and predictions for the model\n",
    "gt_path = 'datasets/MOT17/train/MOT17-02-DPM/gt/gt.txt'\n",
    "predictions_path = 'results/02_DPM_reversed_det.txt'\n",
    "\n",
    "predicted_bbs = []\n",
    "predicted_frame_bbs = [] #frame number of the corresponding predicated box\n",
    "gt_bbs = []\n",
    "gt_frame_bbs = [] #frame number of the corresponding ground true box\n",
    "#Box are structured in the following way x_left,y_top,x_right,y_bottom\n",
    "with open(predictions_path, 'r') as txt_file:\n",
    "  for line in txt_file:\n",
    "    frame, bb_id, bb_left, bb_top, bb_width, bb_height, conf = map(float, line.strip().split(','))\n",
    "    predicted_bbs.append((int(frame),int(bb_id),(bb_left, bb_top, bb_left + bb_width, bb_top + bb_height)))\n",
    "    predicted_frame_bbs.append(frame)\n",
    "with open(gt_path, 'r') as txt_file:\n",
    "  for line in txt_file:\n",
    "    frame, bb_id, bb_left, bb_top, bb_width, bb_height, conf,_,_ = map(float, line.strip().split(','))\n",
    "    gt_bbs.append((int(frame),int(bb_id),(bb_left, bb_top, bb_left + bb_width, bb_top + bb_height)))\n",
    "    gt_frame_bbs.append(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkbH-Zjyj9-P",
    "tags": []
   },
   "source": [
    "### Precision and Recall\n",
    "\n",
    "Let's look into the precision and recall of our model. \n",
    "Here is an example of how you might calculate precision and recall for object detection:\n",
    "- For each image in the test dataset, use your object detection system to generate a set of predicted object bounding boxes.\n",
    "- For each ground truth object in the image, find the predicted object bounding box that has the highest overlap (intersection over union) with the ground truth box.\n",
    "- If the overlap is above a certain threshold (e.g., 0.5), then the predicted object is considered a true positive. Otherwise, it is considered a false positive.\n",
    "- Calculate the precision as the number of true positives divided by the total number of detected objects (true positives + false positives).\n",
    "- Calculate the recall as the number of true positives divided by the total number of true objects (true positives + false negatives).\n",
    "- Repeat this process for all images in the test dataset, and average the precision and recall across all images to get an overall estimate of the object detection system's performance.\n",
    "\n",
    "\n",
    "We then compute the MOTA score of our object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MOTA(path_pred_file, path_gt_file):\n",
    "    df_pred = pd.read_csv(path_pred_file, sep=',', header=None, names=['frame', 'label', 'bb_left', 'bb_top', 'bb_width', 'bb_height','track_id', 'conf'])\n",
    "    df_gt = pd.read_csv(path_gt_file, sep=',', header=None, names=['frame', 'id_nb', 'bb_left', 'bb_top', 'bb_width', 'bb_height', 'conf_score', 'class', 'visibility'])\n",
    "    \n",
    "    # Get only detections of people - no need now that only pedestrian detections are displayed\n",
    "    #df_pred = df_pred[df_pred['label'] == 0]\n",
    "    \n",
    "    # In GT, get detections of people, but also distractors and person in vehicles that might get detected\n",
    "    df_gt = df_gt.loc[df_gt['class'].isin([1,2,7,8,12])]\n",
    "    \n",
    "    \n",
    "    nb_frames = np.max(df_gt['frame'].tolist())\n",
    "    sum_num = 0\n",
    "    sum_denom = 0\n",
    "    precision = []\n",
    "    list_FN = []\n",
    "    list_FP = []\n",
    "    corresp = {}\n",
    "    \n",
    "    for i in tqdm(range(1, nb_frames + 1)):\n",
    "        # Dataframe corresponding to the detections of the current frame i\n",
    "        df_pred_bb = df_pred[df_pred['frame'] == i] \n",
    "        df_gt_bb = df_gt[df_gt['frame'] == i]\n",
    "        \n",
    "\n",
    "        \n",
    "        df_pred_bb = df_pred_bb.reset_index(drop=True)\n",
    "        df_gt_bb = df_gt_bb.reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        # Calculate IoUs between all predicted and ground truth bounding boxes\n",
    "        iou_matrix = np.zeros((len(df_pred_bb.index), len(df_gt_bb.index)))\n",
    "        #print(\"IoU matrix shape: \", iou_matrix.shape)\n",
    "        for j, row_pred in df_pred_bb.iterrows():\n",
    "            for k, row_gt in df_gt_bb.iterrows():\n",
    "                # Convert left, top, width, height coordinates to left, top, right, down\n",
    "                pred_bb = (row_pred['bb_left'], row_pred['bb_top'], row_pred['bb_left'] + row_pred['bb_width'], row_pred['bb_top'] + row_pred['bb_height'])\n",
    "                gt_bb = (row_gt['bb_left'], row_gt['bb_top'], row_gt['bb_left'] + row_gt['bb_width'], row_gt['bb_top'] + row_gt['bb_height'])\n",
    "                iou_matrix[j, k] = compute_iou(pred_bb, gt_bb)\n",
    "        \n",
    "        # Use Hungarian matching to determine optimal matching of predicted to ground truth bounding boxes\n",
    "        row_ind, col_ind = linear_sum_assignment(-iou_matrix)\n",
    "        \n",
    "        FP = 0\n",
    "        matches_true = []\n",
    "        matches_dist = []\n",
    "        list_classes = df_gt_bb['class'].tolist()\n",
    "        list_visibility = df_gt_bb['conf_score'].tolist()\n",
    "        for pred_match, gt_match in zip(row_ind, col_ind):\n",
    "            if iou_matrix[pred_match, gt_match] >= 0.5:\n",
    "                if list_classes[gt_match] == 1 and list_visibility[gt_match] == 1:\n",
    "                    matches_true.append((pred_match, gt_match))\n",
    "                else:\n",
    "                    matches_dist.append((pred_match, gt_match))\n",
    "            else:\n",
    "                FP +=1\n",
    "                \n",
    "        TP = len(matches_true)\n",
    "        \n",
    "        \n",
    "        df_gt_only_target = df_gt_bb[(df_gt_bb['class'] == 1) & (df_gt_bb['conf_score'] == 1)]\n",
    "        \n",
    "        \n",
    "        FN = len(df_gt_only_target.index) - len(matches_true)\n",
    "        GT = len(df_gt_only_target.index)\n",
    "        \n",
    "        list_FN.append(FN)\n",
    "        list_FP.append(FP)\n",
    "        #print(\"True positives:\", TP)\n",
    "        #print(\"False positives:\", FP)\n",
    "        #print(\"False negatives: \", FN)\n",
    "        #print(\"Ground truth: \", GT)\n",
    "        \n",
    "        \n",
    "        sum_num += FN + FP\n",
    "        sum_denom += GT\n",
    "        if (TP+FP != 0):\n",
    "            precision.append(TP/(TP+FP))\n",
    "\n",
    "    MOTA_score = 1 - sum_num/sum_denom\n",
    "    print(\"Final MOTA score: \", MOTA_score)\n",
    "    print(\"Mean Precision @IOU50: \", np.mean(precision))\n",
    "    print(\"Mean False Negatives: \", np.mean(list_FN))\n",
    "    print(\"Mean False Positives: \", np.mean(list_FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 750/750 [00:12<00:00, 59.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MOTA score:  0.46632880948290667\n",
      "Mean Precision @IOU50:  0.8186485674971743\n",
      "Mean False Negatives:  6.101333333333334\n",
      "Mean False Positives:  2.1826666666666665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compute_MOTA('results_proposals_500/13_DPM_reversed_det.txt', 'datasets/MOT17/train/MOT17-13-DPM/gt/gt.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qyAB_17TDiv"
   },
   "source": [
    "## Video analysis in both directions\n",
    "\n",
    "In this section, we try to improve our results by performing the tracking task in both direction and then taking the average position of each box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFAKJzKgnAU_"
   },
   "outputs": [],
   "source": [
    "def tracking_both_direction(frames_folder,video_name,detected_once_threshold,surface_min_iou,video_output=False):\n",
    "  #This function performs the tracking task by taking the analysing the video moving forward and backwards\n",
    "\n",
    "  #First we compute the detection with the frames going forward\n",
    "  forward_det = object_tracking_frame_txt_writing(frames_folder=frames_folder,video_name = \"\",\n",
    "                                  detection_file_name=video_name+'forward_det.txt',reverse=False,video_output=False)\n",
    "  #Then we compute going backwards :\n",
    "  backward_det = object_tracking_frame_txt_writing(frames_folder=frames_folder,video_name = \"\",\n",
    "                                  detection_file_name=video_name+'backwards_det.txt',reverse=True,video_output=False)\n",
    "  \n",
    "  #then we perform the averaging of the two detections\n",
    "  nb_frames=1\n",
    "  for line in forward_det:\n",
    "    if line[0]>nb_frames:\n",
    "      nb_frames = line[0]\n",
    "  average_detections = []\n",
    "  for i in range(nb_frames):\n",
    "      #we extract the detections of the current frame\n",
    "      frames_forward_pred = [cell for cell in forward_det if (cell[0]==i+1 )] #on ne tient pas rigueur de la classe\n",
    "      frames_backward_pred = [cell for cell in backward_det if (cell[0]==i+1 )]\n",
    "\n",
    "      # Calculate IoUs between all predicted and ground truth bounding boxes\n",
    "      iou_matrix = np.zeros((len(frames_forward_pred), len(frames_backward_pred)))\n",
    "      for j, forward_predicted_bb in enumerate(frames_forward_pred):\n",
    "        for k, backward_predicted_bb in enumerate(frames_backward_pred):\n",
    "          iou_matrix[j, k] = compute_iou([forward_predicted_bb[2],forward_predicted_bb[3],forward_predicted_bb[2]+forward_predicted_bb[4],\n",
    "                                          forward_predicted_bb[3]+forward_predicted_bb[5]],\n",
    "                                          [backward_predicted_bb[2],backward_predicted_bb[3],backward_predicted_bb[2]+backward_predicted_bb[4],\n",
    "                                          backward_predicted_bb[3]+backward_predicted_bb[5]])\n",
    "\n",
    "      # Use Hungarian matching to determine optimal matching of predicted to ground truth bounding boxes\n",
    "      row_ind, col_ind = linear_sum_assignment(-iou_matrix)\n",
    "      \n",
    "      for j in range(len(row_ind)):\n",
    "        if(iou_matrix[row_ind[j],col_ind[j]]>=surface_min_iou):\n",
    "          bb_left_av = (frames_forward_pred[row_ind[j]][2] + frames_backward_pred[col_ind[j]][2])/2\n",
    "          bb_top_av = (frames_forward_pred[row_ind[j]][3] + frames_backward_pred[col_ind[j]][3])/2\n",
    "          bb_width_av = (frames_forward_pred[row_ind[j]][4] + frames_backward_pred[col_ind[j]][4])/2\n",
    "          bb_height_av = (frames_forward_pred[row_ind[j]][5] + frames_backward_pred[col_ind[j]][5])/2\n",
    "          score_av = (frames_forward_pred[row_ind[j]][6] + frames_backward_pred[col_ind[j]][6])/2\n",
    "          average_box = [frames_forward_pred[row_ind[j]][0],frames_forward_pred[row_ind[j]][1],bb_left_av,bb_top_av,bb_width_av,bb_height_av,score_av]\n",
    "          average_detections.append(average_box)\n",
    "        else :\n",
    "          average_detections.append(frames_forward_pred[row_ind[j]])\n",
    "          average_detections.append(frames_backward_pred[col_ind[j]])\n",
    "\n",
    "      for j in range(len(frames_forward_pred)):\n",
    "        if j not in row_ind and frames_forward_pred[j][6]>detected_once_threshold:\n",
    "          average_detections.append(frames_forward_pred[j])\n",
    "      for j in range(len(frames_backward_pred)):\n",
    "        if j not in col_ind and frames_backward_pred[j][6]>detected_once_threshold:\n",
    "          average_detections.append(frames_backward_pred[j])\n",
    "      \n",
    "  #We intialize the txt files that writes the boxes detected\n",
    "  with open('/content/drive/MyDrive/DiffusionDet/diffusiondet/tracked_videos/'+ video_name+ 'both_directions.txt', 'w', newline='') as txtfile:\n",
    "    for cell in average_detections:\n",
    "      txtfile.write(str(cell[0])+','+str(cell[1])+','+str(cell[2])+','+str(cell[3])+','+str(cell[4])+','+str(cell[5])+','+str(cell[6])+ '\\n')\n",
    "\n",
    "  #We initialize the output video\n",
    "  if video_output==True:\n",
    "    frames_dir = \"/content/drive/MyDrive/DiffusionDet/diffusiondet/videos/\"+ frames_folder\n",
    "    frames_files_list = [f for f in os.listdir(frames_dir) if f.endswith(\".jpg\")]\n",
    "    frames_files_list.sort()\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_path = os.path.join(\"/content/drive/MyDrive/DiffusionDet/diffusiondet/tracked_videos\", video_name)\n",
    "    frame_0 = cv2.imread(frames_dir + '/' + frames_files_list[0])\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_0.shape[1], frame_0.shape[0]))\n",
    "    for i,frame_name in enumerate(frames_files_list):\n",
    "      frame_detected_bbs = [cell for cell in average_detections if (cell[0]==i+1 )]\n",
    "      frame = cv2.imread(frames_dir+'/'+frame_name)\n",
    "      for bbs in frame_detected_bbs:\n",
    "        cv2.rectangle(frame, (int(bbs[2]), int(bbs[3])), (int(bbs[2]+bbs[4]), int(bbs[3]+bbs[5])), (255, 0, 0), 2)\n",
    "      out.write(frame)\n",
    "    out.release()\n",
    "  return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isqWpd1vt8F3",
    "outputId": "478077be-fb4e-4781-a86a-9169d25ff651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000001.jpg\n",
      "1/20\n",
      "000002.jpg\n",
      "2/20\n",
      "000003.jpg\n",
      "3/20\n",
      "000004.jpg\n",
      "4/20\n",
      "000005.jpg\n",
      "5/20\n",
      "000006.jpg\n",
      "6/20\n",
      "000007.jpg\n",
      "7/20\n",
      "000008.jpg\n",
      "8/20\n",
      "000009.jpg\n",
      "9/20\n",
      "000010.jpg\n",
      "10/20\n",
      "000011.jpg\n",
      "11/20\n",
      "000012.jpg\n",
      "12/20\n",
      "000013.jpg\n",
      "13/20\n",
      "000014.jpg\n",
      "14/20\n",
      "000015.jpg\n",
      "15/20\n",
      "000016.jpg\n",
      "16/20\n",
      "000017.jpg\n",
      "17/20\n",
      "000018.jpg\n",
      "18/20\n",
      "000019.jpg\n",
      "19/20\n",
      "000020.jpg\n",
      "20/20\n",
      "000020.jpg\n",
      "1/20\n",
      "000019.jpg\n",
      "2/20\n",
      "000018.jpg\n",
      "3/20\n",
      "000017.jpg\n",
      "4/20\n",
      "000016.jpg\n",
      "5/20\n",
      "000015.jpg\n",
      "6/20\n",
      "000014.jpg\n",
      "7/20\n",
      "000013.jpg\n",
      "8/20\n",
      "000012.jpg\n",
      "9/20\n",
      "000011.jpg\n",
      "10/20\n",
      "000010.jpg\n",
      "11/20\n",
      "000009.jpg\n",
      "12/20\n",
      "000008.jpg\n",
      "13/20\n",
      "000007.jpg\n",
      "14/20\n",
      "000006.jpg\n",
      "15/20\n",
      "000005.jpg\n",
      "16/20\n",
      "000004.jpg\n",
      "17/20\n",
      "000003.jpg\n",
      "18/20\n",
      "000002.jpg\n",
      "19/20\n",
      "000001.jpg\n",
      "20/20\n"
     ]
    }
   ],
   "source": [
    "tracking_both_direction(frames_folder='MOT17-02-DPM/img1',video_name= \"02_DPM_tracked_both_directions.mp4\",\n",
    "                        detected_once_threshold = 0.4,surface_min_iou=0.5,video_output=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
